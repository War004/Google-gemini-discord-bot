{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvBoob1J1HiZ"
      },
      "source": [
        "# Know Error\n",
        "游릴 The bot tends to genrate multiple responese for a single reply (fixed)\n",
        "\n",
        "游릴 Genrates multiple responses while working with media files (fixed)\n",
        "\n",
        "游릴 Is able to remember images only time it is uploaded.(fixed)\n",
        "\n",
        "游릴 Improving the video responses(Direct function added by google for video file)\n",
        "\n",
        "游릴 Finding a way to keep the converstion for the videos conversation.(Done)\n",
        "\n",
        "游릳 If the user sends a gif through the inbuilt gif section, it's html page is downloaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rJKqNdtYq1g"
      },
      "source": [
        "# Goal\n",
        "1. Fix the 2000 char limit. 九\n",
        "2. Saving the chats. 九\n",
        "3. Only reply when tagged or replied 九\n",
        "4. Consinent chats 九\n",
        "5. Image input 九\n",
        "6. Mutimedia input 九\n",
        "7. Webhook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfsDR_omdNea"
      },
      "source": [
        "# d_bot\n",
        "This notebook lets you run a discord bot powered by google gemini api, with conversation memory which includes audio, video, and images.\n",
        "\n",
        "You can type **\"!check_token\"** to check how many tokens are being used.\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1OQGPc2CsYpnBhNfKEl0O0cDT6uXAcQ8g?usp=sharing\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  \n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "nggWQnMA9EEl",
        "outputId": "de398a9c-34d3-4653-c476-907d39a9755b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<b>Press play on the music player to keep the tab alive, then start block below (Uses only 13MB of data)</b><br/>\n",
              "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title <b>v-- If you play on mobile, tap this to open music player and play the white noise to keep tab running in the background. or your session might get disconnected\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start block below (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X69TeVJU4cmw"
      },
      "outputs": [],
      "source": [
        "# @title Mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFKN43bICGME"
      },
      "source": [
        "# Install requirments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma-s35dvLHdk"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai\n",
        "!pip install Discord\n",
        "!pip install python-magic\n",
        "!pip install nest_asyncio\n",
        "!pip install textract\n",
        "\n",
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "wHUdpD-u1JAm",
        "outputId": "73586afd-fe77-4acc-d038-3222040dcf60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ],
      "source": [
        "# @title Select a model (rerun the 'model configuration' block if you have changed the model in between)\n",
        "# Import the necessary modules\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# List available models and filter them\n",
        "available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
        "\n",
        "# Create a dropdown widget for model selection\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=available_models,\n",
        "    description='Select Model:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "# Display the dropdown widget\n",
        "display(model_dropdown)\n",
        "\n",
        "# Function to get the selected model\n",
        "def get_selected_model(change):\n",
        "    global selected_model\n",
        "    selected_model = change['new']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMwNT2bP1LJm"
      },
      "outputs": [],
      "source": [
        "# Set the event listener for the dropdown change\n",
        "model_dropdown.observe(get_selected_model, names='value')\n",
        "\n",
        "# Default selected model\n",
        "selected_model = model_dropdown.value\n",
        "# @title Model configuration\n",
        "text_generation_config = {\n",
        "    \"temperature\": 0.9,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 0,\n",
        "    \"max_output_tokens\": 8192,\n",
        "}\n",
        "\n",
        "safety_settings = [\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "    \"threshold\": \"BLOCK_NONE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "    \"threshold\": \"BLOCK_NONE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "    \"threshold\": \"BLOCK_NONE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "    \"threshold\": \"BLOCK_NONE\"\n",
        "  },\n",
        "]\n",
        "\n",
        "# Initial prompt\n",
        "system_instruction = \".\" # @param {type:\"string\"}\n",
        "\n",
        "# Create the model using the selected model name from the dropdown\n",
        "model = genai.GenerativeModel(model_name=selected_model, generation_config=text_generation_config, system_instruction=system_instruction, safety_settings=safety_settings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCPt1z07LaCw"
      },
      "outputs": [],
      "source": [
        "# @title Functions\n",
        "\n",
        "from random import choice, randint\n",
        "import os\n",
        "import requests\n",
        "import textract\n",
        "from urllib.parse import urlparse, unquote\n",
        "import re\n",
        "import cv2\n",
        "import shutil\n",
        "import mimetypes\n",
        "import magic\n",
        "import json\n",
        "\n",
        "def extract_text(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts text from a document\n",
        "    \"\"\"\n",
        "    # Extract text from the document\n",
        "    text = textract.process(file_path).decode('utf-8')\n",
        "\n",
        "    return text\n",
        "\n",
        "def download_file(attachment_link: str, user_id: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Downloads the file, determines its type, and renames it with the correct extension.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the URL to get the file name without the extension\n",
        "        parsed_link = urlparse(unquote(attachment_link))\n",
        "        path = parsed_link.path\n",
        "        original_filename = os.path.basename(path).split('?')[0]\n",
        "\n",
        "        # Create a temporary filename with the user_id as prefix and no extension\n",
        "        temp_filename = f'file_{user_id}'\n",
        "        temp_filename_no_ext = temp_filename.rsplit('.', 1)[0]\n",
        "\n",
        "        # Download the file\n",
        "        response = requests.get(attachment_link)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "        # Save the file without an extension\n",
        "        with open(temp_filename_no_ext, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"File downloaded successfully: {temp_filename_no_ext}\")\n",
        "\n",
        "        # Determine the file type and the correct extension\n",
        "        mime_type = determine_file_type(temp_filename_no_ext)\n",
        "        extension = mimetypes.guess_extension(mime_type) or '.bin'  # Default to .bin if unknown\n",
        "\n",
        "        # Rename the file with the correct extension\n",
        "        final_filename = f'{temp_filename_no_ext}{extension}'\n",
        "        os.rename(temp_filename_no_ext, final_filename)\n",
        "        print(f\"File renamed to: {final_filename} with MIME type: {mime_type}\")\n",
        "\n",
        "        return extension, final_filename\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to download file. Error: {e}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def determine_file_type(filepath: str) -> str:\n",
        "    \"\"\"\n",
        "    Determines the MIME type of a file by reading its contents.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the magic library\n",
        "        mime = magic.Magic(mime=True)\n",
        "        mime_type = mime.from_file(filepath)\n",
        "        return mime_type\n",
        "    except Exception as e:\n",
        "        print(f\"Could not determine file type. Error: {e}\")\n",
        "        return 'application/octet-stream'\n",
        "\n",
        "\n",
        "# Function to load chat history from a JSON file\n",
        "def load_chat_history(user_id, custom_file_path):\n",
        "    try:\n",
        "        full_path = f'{custom_file_path}{user_id}_chat_history.json'\n",
        "        with open(full_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        # Populate the chat history from JSON data\n",
        "        user_messages = []\n",
        "        model_responses = []\n",
        "        for entry in data:\n",
        "            role = entry[\"role\"]\n",
        "            parts = entry[\"parts\"]\n",
        "            if role == \"user\":\n",
        "                if len(parts) > 1 and \"file_data\" in parts[1]:\n",
        "                    file_data = parts[1][\"file_data\"]\n",
        "                    user_messages.append({\n",
        "                        \"text\": parts[0][\"text\"],\n",
        "                        \"file_data\": {\n",
        "                            \"mime_type\": file_data[\"mime_type\"],\n",
        "                            \"file_uri\": file_data[\"file_uri\"]\n",
        "                        }\n",
        "                    })\n",
        "                else:\n",
        "                    user_messages.append({\"text\": parts[0][\"text\"], \"file_data\": None})\n",
        "            elif role == \"model\":\n",
        "                model_responses.append(parts[0][\"text\"])\n",
        "\n",
        "        # Fill the chat history with messages and file data using a loop\n",
        "        filled_chat_history = []\n",
        "        for user_content, model_text in zip(user_messages, model_responses):\n",
        "            if user_content.get(\"file_data\"):\n",
        "                user_part = {\n",
        "                    \"parts\": [\n",
        "                        {\"text\": user_content[\"text\"]},\n",
        "                        {\"file_data\": user_content[\"file_data\"]}\n",
        "                    ],\n",
        "                    \"role\": \"user\"\n",
        "                }\n",
        "            else:\n",
        "                user_part = {\"parts\": [{\"text\": user_content[\"text\"]}], \"role\": \"user\"}\n",
        "            model_part = {\"parts\": [{\"text\": model_text}], \"role\": \"model\"}\n",
        "            filled_chat_history.extend([user_part, model_part])\n",
        "        return filled_chat_history\n",
        "    except FileNotFoundError:\n",
        "        return []\n",
        "\n",
        "# Function to save chat history to a JSON file\n",
        "def save_chat_history(user_id, chat, custom_file_path):\n",
        "    chat_history = []\n",
        "    full_path = f'{custom_file_path}{user_id}_chat_history.json'\n",
        "    for entry in chat.history:\n",
        "        entry_values = {'parts': [], 'role': entry.role}\n",
        "        for part in entry.parts:\n",
        "            part_values = {}\n",
        "            if hasattr(part, 'text'):\n",
        "                part_values['text'] = part.text\n",
        "            if hasattr(part, 'file_data'):\n",
        "                part_values['file_data'] = {\n",
        "                    'mime_type': part.file_data.mime_type,\n",
        "                    'file_uri': part.file_data.file_uri\n",
        "                }\n",
        "            entry_values['parts'].append(part_values)\n",
        "        chat_history.append(entry_values)\n",
        "    with open(full_path, 'w') as json_file:\n",
        "        json.dump(chat_history, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPR93kb1_mN8"
      },
      "source": [
        "# Running The Bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYLAjggYGKVB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import aiohttp\n",
        "from typing import Final\n",
        "import os\n",
        "from discord import Intents, Client, Message\n",
        "from google.colab import userdata\n",
        "import PIL.Image\n",
        "import nest_asyncio\n",
        "\n",
        "# STEP 0: LOAD OUR TOKEN FROM SOMEWHERE SAFE\n",
        "TOKEN: Final[str] = userdata.get('DISCORD_TOKEN')\n",
        "\n",
        "# STEP 1: BOT SETUP\n",
        "intents: Intents = Intents.default()\n",
        "intents.message_content = True\n",
        "client: Client = Client(intents=intents)\n",
        "\n",
        "# STEP 2: MESSAGE FUNCTIONALITY\n",
        "async def send_message(message: Message, response: str, user_id: str) -> None:\n",
        "    # Check if the entire response is within the Discord character limit\n",
        "    if len(response) <= 2000:\n",
        "        await message.channel.send(response)\n",
        "    else:\n",
        "        # Initialize the start index of the chunk\n",
        "        start_index = 0\n",
        "        while start_index < len(response):\n",
        "            # Find the end index by looking for the last space before the 2000 character limit\n",
        "            end_index = response.rfind(' ', start_index, start_index + 2000)\n",
        "            # If no space is found, just cut at the 2000 character limit\n",
        "            if end_index == -1:\n",
        "                end_index = start_index + 2000\n",
        "            # Extract the chunk of text\n",
        "            chunk = response[start_index:end_index]\n",
        "            # Send the chunk as a separate message to the channel\n",
        "            await message.channel.send(chunk)\n",
        "            # Update the start index to the end of the last chunk\n",
        "            start_index = end_index + 1  # Skip the space\n",
        "\n",
        "# STEP 3: HANDLING THE STARTUP FOR OUR BOT\n",
        "@client.event\n",
        "async def on_message(message: Message) -> None:\n",
        "    # Ignore messages sent by the bot itself\n",
        "    if message.author == client.user:\n",
        "        return\n",
        "\n",
        "    # Check if the bot is mentioned or if the message is a reply to the bot\n",
        "    bot_mentioned = client.user in message.mentions\n",
        "    is_reply = message.reference is not None and message.reference.resolved.author == client.user\n",
        "\n",
        "    # Regular expression to detect URLs\n",
        "    custom_path = '/content/drive/MyDrive/Discord_bot/'\n",
        "\n",
        "    # Handle !check_token command\n",
        "    if message.content.strip() == \"!check_token\":\n",
        "        user_id: str = str(message.author.id)  # Get the user's ID as a string\n",
        "        chat_history = load_chat_history(user_id, custom_path)\n",
        "        chat = model.start_chat(history=chat_history)\n",
        "        #token_count = model.count_tokens(chat.history)\n",
        "        response = f\"{model.count_tokens(chat.history)}\"\n",
        "        await send_message(message, response, user_id)\n",
        "        return\n",
        "\n",
        "    # Respond only if the bot is mentioned, if it's a reply to the bot's message, or if a URL is detected\n",
        "    if bot_mentioned or is_reply:\n",
        "        user_id: str = str(message.author.id)  # Get the user's ID as a string\n",
        "        username: str = str(message.author)\n",
        "        channel: str = str(message.channel)\n",
        "        url_pattern = re.compile(r'(https?://[^\\s]+)')\n",
        "        urls = url_pattern.findall(message.content)\n",
        "        Direct_upload = False\n",
        "        Link_upload = False\n",
        "        attach_url = None        \n",
        "\n",
        "        # Check for attachments in the message\n",
        "        if urls:\n",
        "            attach_url = urls[0]\n",
        "            Link_upload = True\n",
        "\n",
        "        if message.attachments:\n",
        "            for attachment in message.attachments:\n",
        "                attach_url = attachment.url\n",
        "                Direct_upload = True\n",
        "                break\n",
        "\n",
        "        if Direct_upload or Link_upload:\n",
        "            # Downloading the file while returning the file type\n",
        "            format, downloaded_file = download_file(attach_url, user_id)\n",
        "            if format in ('.pdf', '.docx', '.txt'):\n",
        "                print(f\"({user_id}): {message.content}\")\n",
        "                text = extract_text(downloaded_file)\n",
        "                message.content = f'The user has uploaded a document: {text} The document has ended!! The current user input is: {message.content}'\n",
        "                chat_history = load_chat_history(user_id, custom_path)\n",
        "                chat = model.start_chat(history=chat_history)\n",
        "\n",
        "                response = chat.send_message(message.content)\n",
        "                response = response.text\n",
        "\n",
        "                save_chat_history(user_id, chat, custom_path)\n",
        "                await send_message(message, response, user_id)\n",
        "                print(f\"Bot: {response}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"({user_id}): {message.content}\")\n",
        "                media_file = genai.upload_file(path=downloaded_file)\n",
        "                chat_history = load_chat_history(user_id, custom_path)\n",
        "                chat = model.start_chat(history=chat_history)\n",
        "\n",
        "                response = chat.send_message([message.content, media_file])\n",
        "                response = response.text\n",
        "\n",
        "                save_chat_history(user_id, chat, custom_path)\n",
        "                await send_message(message, response, user_id)\n",
        "                print(f\"Bot: {response}\")\n",
        "            Direct_upload = False\n",
        "            Link_upload = False\n",
        "                        \n",
        "        else:\n",
        "            print(f\"({user_id}): {message.content}\")\n",
        "            chat_history = load_chat_history(user_id, custom_path)\n",
        "            chat = model.start_chat(history=chat_history)\n",
        "\n",
        "            response = chat.send_message(message.content)\n",
        "            response = response.text\n",
        "\n",
        "            save_chat_history(user_id, chat, custom_path)\n",
        "            await send_message(message, response, user_id)\n",
        "            print(f\"Bot: {response}\")\n",
        "\n",
        "# STEP 5: MAIN ENTRY POINT\n",
        "async def main() -> None:\n",
        "    await client.start(TOKEN)\n",
        "\n",
        "# To run in a cloud service like Google Colab, Kaggle, etc.\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the main function\n",
        "await main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
